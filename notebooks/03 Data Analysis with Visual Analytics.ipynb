{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jupytercon/2020-exactlyallan/raw/master/images/RAPIDS-header-graphic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:red;color:white;font:2em\"> Note: It is advised to close the previous notebook kernels and clear GPU Memory in order to avoid GPU Resource Out of Memory errors</p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with Visual Analytics\n",
    "\n",
    "***Combining analytics with visualization***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook we will continue to explore the Divvy bikes dataset using, cuDF, cuGraph, and cuSpatial to see how these analysis results can easily feed directly into visualization tools like hvplot and Datashader.\n",
    "\n",
    "### cuDF, cuGraph, cuSpatial, hvplot, and Datashader\n",
    "- [cuDF](https://docs.rapids.ai/api/cudf/stable/), is a GPU DataFrame library for manipulating data with a pandas-like API.\n",
    "\n",
    "- [cuGraph](https://docs.rapids.ai/api/cugraph/stable/) is a RAPIDS library for GPU accelerated graph library with functionality like NetworkX.\n",
    "\n",
    "- [cuSpatial](https://docs.rapids.ai/api/cuspatial/stable/) is a collection of GPU accelerated algorithms for computing geo-spatial measures.\n",
    "\n",
    "- [hvplot](https://hvplot.holoviz.org/) is a high-level plotting API for the PyData ecosystem built on [HoloViews](http://holoviews.org/).\n",
    "\n",
    "- [Datashader](https://datashader.org/) is a library for high fidelity server side data rendering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In addition to the libraries mentioned above, we will also make use of libraries [cupy](https://docs.cupy.dev/en/stable/), [NumPy](https://numpy.org/), and [Pandas](https://pandas.pydata.org/) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "import cupy\n",
    "import cuspatial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "import hvplot.cudf\n",
    "import hvplot.pandas\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into cuDF\n",
    "\n",
    "First let's load the data. In addition to the main Divvy `data.csv` file, we will also load the small `stations.csv` file that we prepared in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: remember to reparse into datetimes\n",
    "df = cudf.read_csv(DATA_DIR / \"data.csv\", parse_dates=('starttime', 'stoptime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(DATA_DIR / \"stations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to continue our investigation into weekday vs weekend patterns, so let's first add a column for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df['starttime'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Analysis with CuSpatial \n",
    "\n",
    "Let's take a look at some spatial measures and see if there are any interesting features.\n",
    "\n",
    "We might start with the first station, and see what the max trip length from it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = df.iloc[0]\n",
    "station_id, origin_lon, origin_lat = r0[\"from_station_id\"], r0[\"longitude_start\"], r0[\"latitude_start\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cuSpatial function `lonlat_to_cartesian` will let us quickly compute the x/y distances for every ending trip location (in Kilometers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[df[\"from_station_id\"]==station_id[0]]\n",
    "dist = cuspatial.lonlat_to_cartesian(origin_lon[0], origin_lat[0], sub_df[\"longitude_end\"], sub_df[\"latitude_end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CuPy functions can compute derived values on these GPU dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good o' pythagorean theorem\n",
    "cupy.sqrt(cupy.max(dist.x**2 + dist.y**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to compute all trip distances? We can compute the distances using every station as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_dists(df):\n",
    "    results = []\n",
    "\n",
    "    for idx, row in stations.iterrows():\n",
    "        station_id, origin_lon, origin_lat = int(row[\"station_id\"]), row[\"lon\"], row[\"lat\"]\n",
    "        sub_df = df[df[\"from_station_id\"]==station_id]\n",
    "        res = cuspatial.lonlat_to_cartesian(origin_lon, origin_lat, sub_df[\"longitude_end\"], sub_df[\"latitude_end\"])\n",
    "        res[\"dist\"] = cupy.sqrt(res.x**2 + res.y**2)\n",
    "        results.append(res)\n",
    "        \n",
    "    return cudf.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_from_dists = trip_dists(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hvplot of Trip Distances\n",
    "Now that we have all the distances in a dataframe, we can use hvplot to create a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bin size is chosen after some trial and error\n",
    "all_from_dists.hvplot.hist(y=\"dist\", normed=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly most trips are fairly short -usually under 2Km. This makes sense when we remember most trip durations are also less than 15min. \n",
    "\n",
    "It might also be interesting follow up and break the distribution of trips down weekday vs weekend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_trips = df[df[\"weekday\"].isin([5, 6])] # weekend days = 5, 6 \n",
    "weekday_trips = df[df[\"weekday\"].isin(list(range(5)))]  # weekday days = 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distances from the previous function\n",
    "weekend_dists = trip_dists(weekend_trips)\n",
    "weekday_dists = trip_dists(weekday_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combined_dists =  cudf.concat([weekday_dists, weekend_dists])\n",
    "all_combined_dists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hvPlot of Weekend vs Weekday Trip Distance\n",
    "Plotting these two distributions together we can see the weekday (orange) trips peak more at shorter distances and the weekend distributions (blue) has more, longer trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekend_hist = weekend_dists.hvplot.hist(y=\"dist\", alpha=0.5, bin_range=(0, 10), normed=True, color=\"blue\")\n",
    "weekday_hist = weekday_dists.hvplot.hist(y=\"dist\", alpha=0.5, bin_range=(0, 10), normed=True, color=\"orange\")\n",
    "weekend_hist * weekday_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While interesting to note, there doesn't appear to be any major revelations using a distance analysis approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Analysis with cuDF\n",
    "\n",
    "Let's use CuDF direclty to group and aggregate our data to look for anyting intersting about the flow of trips in and out stations. \n",
    "\n",
    "We want to look at the daily net flow of trips at each station, i.e. how many more (or less) trips *started* at a station vs *ended* at a station in a given day.\n",
    "\n",
    "In order to group by day, we first take the \"floor\" of each timestamp divided by one day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = np.datetime64(1, 'D').astype('datetime64[ns]').astype('int64') \n",
    "\n",
    "# out\n",
    "df['from_day'] = df['starttime'].astype('int64') // one_day\n",
    "\n",
    "# in\n",
    "df['to_day'] = df['stoptime'].astype('int64') // one_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can group by the station id and hour for both the departing and arriving cases. We name the columns from the size DataFrame `out` and `in` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.groupby(by=[\"from_station_id\", \"from_day\"]).size().to_frame('out').reset_index()\n",
    "df_in = df.groupby(by=[\"to_station_id\", \"to_day\"]).size().to_frame('in').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rename the columns to be the same in both DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.rename(columns={\"from_station_id\": \"station_id\", \"from_day\": \"day\"}, inplace=True)\n",
    "df_in.rename(columns={\"to_station_id\": \"station_id\", \"to_day\": \"day\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reset the index to be the (station id, hour) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.set_index([\"station_id\", \"day\"])\n",
    "df_in = df_in.set_index([\"station_id\", \"day\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join these two DataFrames to compute an `flow = out - in` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df_in.join(df_out, how=\"outer\").fillna(0).reset_index()\n",
    "full_df[\"flow\"] = full_df[\"out\"] - full_df[\"in\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also convert our \"day\" values back to proper timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"time\"] = (full_df[\"day\"] * one_day).astype('datetime64[ns]')\n",
    "full_df = full_df[[\"station_id\", \"time\", \"flow\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a glimpse at the resulting DataFrame which has the net `out-in` trip flow by station per day:\n",
    "- A `+` positive number means there was an excess of trips *starting out* of the station that day.\n",
    "- A `-` negative number means an excess of trips *ending in* the station that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might like to look at the maximal behaviour. What is a high number of excess arrivals or departures at a station? Let's pull out individual timeseries for each station id, and look a the max/min for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = []\n",
    "for i in stations.station_id:\n",
    "    subdf = full_df[full_df.station_id==i].set_index(\"time\")\n",
    "    flows.append((i, subdf.flow.max(), subdf.flow.min()))\n",
    "flows = pd.DataFrame(flows, columns=[\"station_id\", \"max_out\", \"max_in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information, we can see what stations had the largest ever excess departures (station 192) or arrivals (station 77):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_out.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_in.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing about excess arrivals vs departures is probably important for Divvy to be able to manually re-allocate bikes. We could ask what fraction of stations ever have a max of more than 30 excess trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(flows[flows.max_out > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flows[flows.max_in < -30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While looking at individual stations or max/mins is useful to get preliminary ideas of patterns, it would be better to see it all at once. First we need to prepare a new Dataframe that has all the series as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = []\n",
    "\n",
    "for i in stations.station_id:\n",
    "    s = full_df[full_df.station_id==i][[\"time\", \"flow\"]]\n",
    "    s.rename(columns={\"flow\": f\"s{i}\"}, inplace=True)\n",
    "    s = s.set_index(\"time\")\n",
    "    series.append(s)\n",
    "    \n",
    "df_wide = cudf.concat(series, axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Dataframe has a daily time series for every column, one for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hvplot of Select Station Flows\n",
    "It's simple to pull out individual stations for comparison using `hvplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[\"s77\",\"s81\",\"s192\",\"s195\",\"s268\",\"s287\"], alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows some of the more interesting station patterns - which roughly match the overall seasonal flows. Station 195 appears perpetually over taxed, while something nearby station 77 seems to draw in a lot of bikes. Yet its hard to gleen a pattern without the connection between station's flows. \n",
    "\n",
    "Bonus points for anyone who knows what anomaly happened on 6/24/2014 (seriously, we're curious). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It Now\n",
    "See if you can plot all the stations in an hvplot (it is possible but takes a while to render): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets take a look at the data with Datashader. First we make a funtion `series_shade` that can take a wide dataframe of timeseries like we have made above, and render *all* of the series at once using Datashader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details here https://datashader.org/user_guide/index.html\n",
    "def series_shade(df):\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    itime = cudf.to_datetime(df.index).astype('int64')\n",
    "    x_range = (itime[0], itime[-1])\n",
    "    \n",
    "    y_range = (df.min().min(), df.max().max())\n",
    "    \n",
    "    temp = cudf.DataFrame(df)\n",
    "    temp[\"itime\"] = itime\n",
    "    \n",
    "    # the width is 4x365, leaving one pixel width per day\n",
    "    cvs = ds.Canvas(plot_height=400, plot_width=1460)\n",
    "    agg = cvs.line(temp, x=\"itime\", y=cols, agg=ds.count(), axis=1)\n",
    "    \n",
    "    print(f\"y range: ({y_range[0]}, {y_range[1]})\")\n",
    "    return tf.shade(agg, how='linear', cmap=[\"purple\",\"red\",\"white\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashader Line Plot of Total Daily Flows\n",
    "Now let's pass in-out daily net excess data to get a rough datashder plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "series_shade(df_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not completely clear what we can see here, but it points to some ideas for future exploration. If you squint it does seem that there is an unbalanced flow out of stations vs into stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashader Line Plot of Cumulative Daily Flows\n",
    "As a last experiment, let's make the same plot, but with *cumulative* excess trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cumulative = df_wide.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_shade(df_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This view emphasizes the unbalanced flow and is a bit more interesting. It illustrates the notion that Divvy must be engaging in a lot of continual re-allocation of its bikes to offset these excess trips at individual stations.\n",
    "\n",
    "If we knew the marginal costs compared to ridership income, it could prove an interesting data point on when network expansion would become prohibitive. However, without that we need to look elsewhere for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It Now\n",
    "Datashader plots can be wrapped in hvplots, much like bokeh plots. Try wrapping the above examples in order to make them more interactive by using [Datashader's usage guide](https://datashader.org/user_guide/Timeseries.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Analysis with cuGraph PageRank\n",
    "\n",
    "In our previous notebook we were able to find some interesting patterns by converting our dataframe into a graph. Here, we will try the `cugraph.pagerank` algorithm to see if it helps succinctly illustrate patterns for the \"most popular\" stations.\n",
    "\n",
    "First, let's see what it looks like to compute PageRank for a single hour of the day, e.g. 5PM, by subsetting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17 = df[df[\"hour\"]==17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then groupby (from_station_id, to_station_id) and take the group size to get all the unique individual routes between stations that hour, and also the number of trips that took each of those routes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g17 = df.groupby(by=[\"from_station_id\", \"to_station_id\"])\n",
    "routes17 = g17.size().reset_index()\n",
    "routes17.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a `cugraph.Graph` with the from and to station IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(d17, source='from_station_id', destination='to_station_id')\n",
    "d17_page = cugraph.pagerank(G)\n",
    "d17_page.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PageRank values are relative, and as such do not matter as much as the ranking it produces for the network of trips. Let's see which stations rank as most important at 5PM (on any day):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_top = d17_page.nlargest(20, \"pagerank\").to_pandas()\n",
    "d17_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hvplot of 5pm Top PageRank Locations\n",
    "Plotting these stations we can see that at 5PM the most important stations are nearly all downtown, matching our previous notebook findings about a focused downtown core of total trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_page_locs = stations[stations.station_id.isin(d17_top.vertex)]\n",
    "d17_page_locs.hvplot.points(x='lon', y='lat', alpha=0.7, size=300, geo=True, tiles=\"OSM\").opts(width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know applying PageRank seems to produce useful results, let's look at how stations rank by weekdays vs weekends. To get proper rankings, we need to compute it for every individual day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for w in range(7):\n",
    "    dfw = df[df[\"weekday\"]==w]\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(dfw, source='from_station_id', destination='to_station_id')\n",
    "    df_page = cugraph.pagerank(G).nlargest(20, \"pagerank\")\n",
    "    results[w] = set(df_page.to_pandas()[\"vertex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out what stations were continually highest ranked among all weekdays and weekend days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = set.intersection(*[results[i] for i in range(5)]) # days 0-4 are weekdays\n",
    "weekend = set.intersection(results[5], results[6])  # days 5-6 are the weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the stations that are ranked important on weekdays and ranked important on weekends, we find that there is little overlap:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot these quickly using hvplot again. Let's add a column to denote weekday / weekend so that we can group by that type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = stations[stations.station_id.isin(weekend)]\n",
    "r1 = r1.assign(type=\"Weekend\")\n",
    "\n",
    "r2 = stations[stations.station_id.isin(weekday)]\n",
    "r2 = r2.assign(type=\"Weekday\")\n",
    "\n",
    "result = pd.concat([r1, r2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hvplot of Weekend / Weekday Top PageRank Locations\n",
    "Looking at the plot, nearly all the important weekday stations are downtown, and on the weekend the important stations are further out in popular districts around downtown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.hvplot.points(x='lon', y='lat', by='type', \n",
    "                     alpha=0.7, size=300, geo=True, tiles=\"OSM\").opts(width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above map of top PageRanked stations for weekend / weekdays matches very well with the ForceAtlas2 clustered graph and time series cross-filtered visualizations of the previous notebook, but in a much more concise and presentable manner. This is the positive result we were hoping to find with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Analytics Findings \n",
    "When running analytics, its critical to have a solid understanding of the underlying data in order to make correct decisions. We tried several analytical approaches to see if we could glean some meaningful patterns. As is often the case, some worked better than others - but because we did extensive exploratory visualization we now have confidence that the weekend / weekday binned PageRank approach will produce accurate results when used for visualizations in our next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
